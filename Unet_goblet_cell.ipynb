{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "colab": {
      "name": "Unet_goblet_cell.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "downgrade tensorflow 2.2.0 -> 1.14.0",
      "metadata": {
        "id": "WQTNeQFln3ss",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "source": "!pip install tensorflow==1.14.0",
      "metadata": {
        "id": "kfubCztKglBQ",
        "colab_type": "code",
        "outputId": "f385ce6e-be3b-4278-a8f1-393df57e23a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/f0/96fb2e0412ae9692dbf400e5b04432885f677ad6241c088ccc5fe7724d69/tensorflow-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (109.2MB)\n",
            "\u001b[K     |████████████████████████████████| 109.2MB 37kB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.9.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.12.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.3.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.12.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (3.10.0)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 49.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.34.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.18.3)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 44.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.0.8)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.28.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.8.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.14.0) (46.1.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.2.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14.0) (2.10.0)\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: tensorboard 2.2.1\n",
            "    Uninstalling tensorboard-2.2.1:\n",
            "      Successfully uninstalled tensorboard-2.2.1\n",
            "  Found existing installation: tensorflow-estimator 2.2.0\n",
            "    Uninstalling tensorflow-estimator-2.2.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0\n",
            "  Found existing installation: tensorflow 2.2.0rc3\n",
            "    Uninstalling tensorflow-2.2.0rc3:\n",
            "      Successfully uninstalled tensorflow-2.2.0rc3\n",
            "Successfully installed tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0\n"
          ],
          "name": "stdout"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Mon Apr  6 13:54:17 2020\n\n@author: Taehan Kim\n\"\"\"\n\n\n\n\nimport glob #file directory\nimport shutil\nimport os\nimport random\nfrom PIL import Image\nimport seaborn as sns #statistical data visualization\nimport pandas as pd\nimport pandas.util.testing as tm\nimport numpy as np\nfrom sklearn.model_selection import train_test_split,StratifiedKFold\nfrom tensorflow.keras.layers import Dense, Input, Activation, Flatten\nfrom tensorflow.keras.layers import BatchNormalization,Add,Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.layers import LeakyReLU, ReLU, Conv2D, MaxPooling2D, BatchNormalization, Conv2DTranspose, UpSampling2D, concatenate\nfrom tensorflow.keras import callbacks\nfrom tensorflow.keras import backend as K\n\n\n",
      "metadata": {
        "id": "rlqIYlWhPWNS",
        "colab_type": "code",
        "outputId": "049efdba-bf17-4e38-ffea-0df0036dca08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Google Drive에 마운트\n",
      "metadata": {
        "id": "ZMcJorxnOTGj",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "source": "from google.colab import drive\ndrive.mount('/content/gdrive')",
      "metadata": {
        "id": "dTWAmc4FtNuU",
        "colab_type": "code",
        "outputId": "6160556c-3dd7-44af-f100-0ad561b4a099",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#%% mask_df는 정답 마스크에 대해 정의 하고 이를 정리해놓은 변수\nmask_filenames = glob.glob('/content/gdrive/My Drive/Colab Notebooks/masks/*')\nmask_df = pd.DataFrame()\nmask_df['filename'] = mask_filenames\nmask_df['mask_percentage'] = 0\nmask_df['labels'] = 0\nmask_df.set_index('filename', inplace=True)\n\nfor file in mask_filenames:\n    mask_df.loc[file, 'mask_percentage'] = np.array(Image.open(file)).sum()/(128*128)\n\nmask_df.loc[mask_df.mask_percentage > 0, 'labels'] = 1\n#%% \ntrain_valid_filenames = glob.glob('/content/gdrive/My Drive/Colab Notebooks/train/*')\n#전부 마스크에 대한 변수가 됩니다. \ntrain_filenames, valid_filenames = train_test_split(train_valid_filenames, stratify = mask_df.labels, test_size = 0.2, random_state = 10)\nmask_train_filenames = [f.replace('/train', '/masks') for f in train_filenames]\nmask_valid_filenames = [f.replace('/train', '/masks') for f in valid_filenames]\n# from efficientnet import EfficientNetB0",
      "metadata": {
        "id": "v3lsVqYSP3Xi",
        "colab_type": "code",
        "colab": {}
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#%%\ntrain_x = np.zeros((len(train_filenames),128,128))\nvalid_x = np.zeros((len(valid_filenames),128,128))\ntrain_mask_y = np.zeros((len(train_filenames),128,128))\nvalid_mask_y = np.zeros((len(valid_filenames),128,128))\ntrain_y = np.zeros((len(train_filenames)))\nvalid_y = np.zeros((len(valid_filenames)))\n\n# 훈련용 .train image\nfor (index, image) in enumerate(train_filenames[:]): #index와 내용을 함께\n    train_x[index] = np.array(Image.open(image))\n    \n# 검증용 및 변병여부 .train image\nfor (index, image) in enumerate(valid_filenames[:]):\n    valid_x[index] = np.array(Image.open(image))\n    \n# 훈련용 마스크 답지 및 병변여부.mask image\nfor (index, image) in enumerate(mask_train_filenames[:]):\n    train_mask_y[index] = np.array(Image.open(image))\n    train_y[index] = mask_df.loc[image, 'labels']\n    \n# 검증용 병변 답지 및 병변여부 .mask image\nfor (index, image) in enumerate(mask_valid_filenames[:]):\n    valid_mask_y[index] = np.array(Image.open(image))\n    valid_y[index] = mask_df.loc[image, 'labels']",
      "metadata": {
        "id": "YHrGVsTMP8wl",
        "colab_type": "code",
        "colab": {}
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "train_x = train_x.reshape(len(train_filenames),128,128,1)\nvalid_x = valid_x.reshape(len(valid_filenames),128,128,1)\n\ntrain_mask_y = train_x.reshape(len(train_filenames),128,128,1)\nvalid_mask_y = valid_x.reshape(len(valid_filenames),128,128,1)",
      "metadata": {
        "id": "ln_Cx2N4QBKs",
        "colab_type": "code",
        "colab": {}
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "def UNet(pretrained_weights = None,input_size = (128,128,1)):\n    inp = Input(input_size)\n    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inp)\n    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n    \n    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n    \n    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n    \n    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n    drop4 = Dropout(0.2)(conv4)\n    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n\n    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n    drop5 = Dropout(0.2)(conv5)\n\n    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n    merge6 = concatenate([drop4,up6], axis = 3)\n    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n\n    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n    merge7 = concatenate([conv3,up7], axis = 3)\n    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n\n    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n    merge8 = concatenate([conv2,up8], axis = 3)\n    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n\n    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n    merge9 = concatenate([conv1,up9], axis = 3)\n    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n\n    model = Model(inputs = inp, outputs=[conv10])\n\n    return model",
      "metadata": {
        "id": "P_FDJujwQC_7",
        "colab_type": "code",
        "colab": {}
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "model = UNet()\nmodel.summary()\nmodel.compile(optimizer = Adam(lr = 1e-3), loss = 'binary_crossentropy', metrics = ['accuracy'])",
      "metadata": {
        "id": "JGJwK_wDQF1J",
        "colab_type": "code",
        "outputId": "743f043e-88b3-42a1-8433-0de8992bf748",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 128, 128, 1) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 128, 128, 64) 640         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 128, 128, 64) 36928       conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 64, 64, 64)   0           conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 64, 64, 128)  73856       max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 64, 64, 128)  147584      conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, 32, 32, 128)  0           conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 32, 32, 256)  295168      max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 32, 32, 256)  590080      conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2D)  (None, 16, 16, 256)  0           conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 16, 16, 512)  1180160     max_pooling2d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 16, 16, 512)  2359808     conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 16, 16, 512)  0           conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2D)  (None, 8, 8, 512)    0           dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 8, 8, 1024)   4719616     max_pooling2d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 8, 8, 1024)   9438208     conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 8, 8, 1024)   0           conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_4 (UpSampling2D)  (None, 16, 16, 1024) 0           dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 16, 16, 512)  2097664     up_sampling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 16, 16, 1024) 0           dropout_2[0][0]                  \n",
            "                                                                 conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 16, 16, 512)  4719104     concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 16, 16, 512)  2359808     conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_5 (UpSampling2D)  (None, 32, 32, 512)  0           conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 32, 32, 256)  524544      up_sampling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 32, 32, 512)  0           conv2d_29[0][0]                  \n",
            "                                                                 conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 32, 32, 256)  1179904     concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 32, 32, 256)  590080      conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_6 (UpSampling2D)  (None, 64, 64, 256)  0           conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 64, 64, 128)  131200      up_sampling2d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 64, 64, 256)  0           conv2d_27[0][0]                  \n",
            "                                                                 conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 64, 64, 128)  295040      concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 64, 64, 128)  147584      conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_7 (UpSampling2D)  (None, 128, 128, 128 0           conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 128, 128, 64) 32832       up_sampling2d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 128, 128, 128 0           conv2d_25[0][0]                  \n",
            "                                                                 conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 128, 128, 64) 73792       concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 128, 128, 64) 36928       conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 128, 128, 2)  1154        conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 128, 128, 1)  3           conv2d_46[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 31,031,685\n",
            "Trainable params: 31,031,685\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "history = model.fit(train_x/65535, [train_mask_y/65535],\n                                    epochs = 10, \n                                    batch_size = 256,\n                                    verbose = 1, \n                                    validation_data=(valid_x/65535,[valid_mask_y/65535]))",
      "metadata": {
        "id": "oMzwuYWcQIEO",
        "colab_type": "code",
        "outputId": "b9edaffb-d5c7-4c2f-8187-07bf80a23616",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 736
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 2252 samples, validate on 564 samples\n",
            "Epoch 1/10\n",
            "2252/2252 [==============================] - 2313s 1s/sample - loss: 7.5917 - acc: 0.0140 - val_loss: 0.6600 - val_acc: 0.0134\n",
            "Epoch 2/10\n",
            "2252/2252 [==============================] - 2345s 1s/sample - loss: 0.6092 - acc: 0.0140 - val_loss: 0.5265 - val_acc: 0.0134\n",
            "Epoch 3/10\n",
            "2252/2252 [==============================] - 2331s 1s/sample - loss: 0.4863 - acc: 0.0140 - val_loss: 0.4093 - val_acc: 0.0134\n",
            "Epoch 4/10\n",
            "2252/2252 [==============================] - 2344s 1s/sample - loss: 0.4118 - acc: 0.0140 - val_loss: 0.4123 - val_acc: 0.0134\n",
            "Epoch 5/10\n",
            "2252/2252 [==============================] - 2337s 1s/sample - loss: 0.4010 - acc: 0.0140 - val_loss: 0.3961 - val_acc: 0.0134\n",
            "Epoch 6/10\n",
            "2252/2252 [==============================] - 2355s 1s/sample - loss: 0.3935 - acc: 0.0140 - val_loss: 0.3940 - val_acc: 0.0134\n",
            "Epoch 7/10\n",
            "2252/2252 [==============================] - 2361s 1s/sample - loss: 0.3902 - acc: 0.0140 - val_loss: 0.3924 - val_acc: 0.0134\n",
            "Epoch 8/10\n",
            "2252/2252 [==============================] - 2359s 1s/sample - loss: 0.3887 - acc: 0.0140 - val_loss: 0.3912 - val_acc: 0.0134\n",
            "Epoch 9/10\n",
            "2252/2252 [==============================] - 2367s 1s/sample - loss: 0.3880 - acc: 0.0140 - val_loss: 0.3907 - val_acc: 0.0134\n",
            "Epoch 10/10\n",
            " 256/2252 [==>...........................] - ETA: 33:23 - loss: 0.4008 - acc: 0.0086"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-a2a1a68d622b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                                     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                     \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                                     validation_data=(valid_x/65535,[valid_mask_y/65535]))\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Accuracy & Loss visualization",
      "metadata": {
        "id": "FvKmL-5PBDrK",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "source": "from matplotlib import pyplot as plt\n\nprint(history.history.keys())\n# summarize history for accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n \n",
      "metadata": {
        "id": "rrm-Jv-B58nV",
        "colab_type": "code",
        "colab": {}
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}